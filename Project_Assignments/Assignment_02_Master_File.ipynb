{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee1acc6",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77745f5d",
   "metadata": {},
   "source": [
    "# Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "932c1928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#Install packege of selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bbbfc",
   "metadata": {},
   "source": [
    "# Question no 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e335fc2",
   "metadata": {},
   "source": [
    "#### Q :-\n",
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68b324b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Libraries.\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the \"naukri \" page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b00e2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entering \"designation keywords\"  , as required in the question-\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Analyst\")\n",
    "\n",
    "# Entering \"location keywords\" , as required in the question-\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "# Now we write a command , to click on the submit button.\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "# Here we created an Empty List , to store the Elements receiving after iterating any code or loops.\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56cb83e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# Now we create a User-Defined function, so that it may call it,  when we require its usage.\n",
    "def srapingnaukridata():\n",
    "    # scraping job title from the given page\n",
    "    title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in title_tags[0:10]:\n",
    "        title=i.text\n",
    "        job_title.append(title)\n",
    "    \n",
    "    # scraping job location from the given page\n",
    "    location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "    for i in location_tags[0:10]:\n",
    "        location=i.text\n",
    "        job_location.append(location)\n",
    "    \n",
    "    # scraping company name from the given page\n",
    "    company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in company_tags[0:10]:\n",
    "        company=i.text\n",
    "        company_name.append(company)\n",
    "    \n",
    "    # scraping job experience from the given page\n",
    "    experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 expwdth\"]')\n",
    "    for i in experience_tags[0:10]:\n",
    "        experience=i.text\n",
    "        experience_required.append(experience)\n",
    "\n",
    "# Calling User-defined fn.\n",
    "srapingnaukridata()\n",
    "\n",
    "# checking the length of the list, & store the Elements ; with in the Empty list created above.\n",
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b74cd500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SERIAL_NUM</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Namdhari Seeds</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Senior Data Analyst - Applied Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business Data Analysts</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Zscaler Softech</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Business Data Analysts</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Zscaler Softech</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Permanent Remote</td>\n",
       "      <td>Treebo Hotels</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Assistant Manager/Manager - Data Analyst - Ana...</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Business Analyst/ Sr. Data Analyst</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru(Whitefield)</td>\n",
       "      <td>Zen Data Shastra Llp</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Data Analyst 1</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Inspira Enterprise India</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SERIAL_NUM                                          JOB_TITLE  \\\n",
       "0           1                                       Data Analyst   \n",
       "1           2            Senior Data Analyst - Applied Analytics   \n",
       "2           3                             Business Data Analysts   \n",
       "3           4                             Business Data Analysts   \n",
       "4           5                                       Data Analyst   \n",
       "5           6  Assistant Manager/Manager - Data Analyst - Ana...   \n",
       "6           7                 Business Analyst/ Sr. Data Analyst   \n",
       "7           8                                       Data Analyst   \n",
       "8           9                                     Data Analyst 1   \n",
       "9          10                                   Sr. Data Analyst   \n",
       "\n",
       "                               JOB_LOCATION               COMPANY_NAME  \\\n",
       "0                       Bangalore/Bengaluru             Namdhari Seeds   \n",
       "1                       Bangalore/Bengaluru                        IBM   \n",
       "2                       Bangalore/Bengaluru            Zscaler Softech   \n",
       "3                       Bangalore/Bengaluru            Zscaler Softech   \n",
       "4                          Permanent Remote              Treebo Hotels   \n",
       "5          Bangalore/Bengaluru, Delhi / NCR  Huquo Consulting Pvt. Ltd   \n",
       "6  Hybrid - Bangalore/Bengaluru(Whitefield)       Zen Data Shastra Llp   \n",
       "7                       Bangalore/Bengaluru                      Optum   \n",
       "8                       Bangalore/Bengaluru                      Optum   \n",
       "9                       Bangalore/Bengaluru   Inspira Enterprise India   \n",
       "\n",
       "    EXP_REQ  \n",
       "0   0-2 Yrs  \n",
       "1   2-6 Yrs  \n",
       "2  5-10 Yrs  \n",
       "3  5-10 Yrs  \n",
       "4   1-2 Yrs  \n",
       "5  6-11 Yrs  \n",
       "6   2-6 Yrs  \n",
       "7   2-5 Yrs  \n",
       "8   5-7 Yrs  \n",
       "9   3-4 Yrs  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After confirmation from the above code run , and its equivalent outputs in all the fields.\n",
    "# Now , We move towards making of its data Frame structure.\n",
    "\n",
    "df=pd.DataFrame({'SERIAL_NUM':range(1,11,1),'JOB_TITLE':job_title,'JOB_LOCATION':job_location,'COMPANY_NAME':company_name,'EXP_REQ':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8b0c6f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df4f534",
   "metadata": {},
   "source": [
    "# Question no 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f14113",
   "metadata": {},
   "source": [
    "### Q :-\n",
    "\n",
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f205dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Libraries.\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "     \n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.digit.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6de34d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering \"designation keywords\"  , as required in the question-\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Entering \"location keywords\" , as required in the question-\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "# Now we write a command , to click on the submit button.\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "# Here we created an Empty List , to store the Elements receiving after iterating any code or loops.\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a14cd2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "# Now we create a User-Defined function, so that it may call it,  when we require its usage.\n",
    "def srapingnaukridata2():\n",
    "    \n",
    "    # scraping job title from the given page\n",
    "    title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in title_tags[0:10]:\n",
    "        title=i.text\n",
    "        job_title.append(title)\n",
    "    \n",
    "    # scraping job location from the given page\n",
    "    location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "    for i in location_tags[0:10]:\n",
    "        location=i.text\n",
    "        job_location.append(location)\n",
    "    \n",
    "    # scraping company name from the given page\n",
    "    company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in company_tags[0:10]:\n",
    "        company=i.text\n",
    "        company_name.append(company)\n",
    "        \n",
    "# Calling User-defined fn.\n",
    "srapingnaukridata2()\n",
    "\n",
    "# checking the length of the list, & store the Elements ; with in the Empty list created above.\n",
    "print(len(job_title),len(job_location),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81ad269d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SERIAL_NUM</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Noida, Hyderabad...</td>\n",
       "      <td>Birlasoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Manager</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Senior Principal Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>DATA SCIENTIST III</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...</td>\n",
       "      <td>Mindtree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Baker Hughes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Infosys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SERIAL_NUM                         JOB_TITLE  \\\n",
       "0           1                    Data Scientist   \n",
       "1           2  Analystics & Modeling Specialist   \n",
       "2           3                           Manager   \n",
       "3           4   Senior Principal Data Scientist   \n",
       "4           5          Principal Data Scientist   \n",
       "5           6             Senior Data Scientist   \n",
       "6           7                DATA SCIENTIST III   \n",
       "7           8                    Data Scientist   \n",
       "8           9             Senior Data Scientist   \n",
       "9          10                    Data Scientist   \n",
       "\n",
       "                                        JOB_LOCATION  COMPANY_NAME  \n",
       "0  Hybrid - Bangalore/Bengaluru, Noida, Hyderabad...     Birlasoft  \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...     Accenture  \n",
       "2  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...           PwC  \n",
       "3                                Bangalore/Bengaluru         Optum  \n",
       "4                                Bangalore/Bengaluru         Optum  \n",
       "5                                Bangalore/Bengaluru         Optum  \n",
       "6                                Bangalore/Bengaluru       Walmart  \n",
       "7  Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...      Mindtree  \n",
       "8                        Bangalore/Bengaluru, Mumbai  Baker Hughes  \n",
       "9                                Bangalore/Bengaluru       Infosys  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After confirmation from the above code run , and its equivalent outputs in all the fields.\n",
    "# Now , We move towards making of its data Frame structure.\n",
    "\n",
    "df=pd.DataFrame({'SERIAL_NUM':range(1,11,1),'JOB_TITLE':job_title,'JOB_LOCATION':job_location,'COMPANY_NAME':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71cb5d26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecf8922",
   "metadata": {},
   "source": [
    "# Question no 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44981739",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a917919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required , Libraries.\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb237de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Searching the search_Icon and giving a click command to it.\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "# creating Empty list's.\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6981dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching & applying the filter , location as \"Delhi/NCR\" .\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ca68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching & applying the filter , Salary as \"3-6\" \n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0b1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "# scraping job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 expwdth\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5683341a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SERIAL_NUM</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Artificial Intelligence/Computer Vision Engine...</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Vicara</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Activation Specialist - Adobe Target</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Navikenz India</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Temp. WFH - Noida</td>\n",
       "      <td>NGI Ventures</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...</td>\n",
       "      <td>torcai digital media</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Opening For Jr. Data Scientist with Tatras Dat...</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>Tatras Data Services</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Americana Restaurants (india)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>URGENT: Data Scientist | Gurugram | 5 Days Wor...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Digilytics</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Alliance Recruitment Agency</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SERIAL_NUM                                          JOB_TITLE  \\\n",
       "0           1  Artificial Intelligence/Computer Vision Engine...   \n",
       "1           2          Data Activation Specialist - Adobe Target   \n",
       "2           3                  Data Scientist - Engine Algorithm   \n",
       "3           4                           Associate Data Scientist   \n",
       "4           5                                     Data Scientist   \n",
       "5           6                                     Data Scientist   \n",
       "6           7  Opening For Jr. Data Scientist with Tatras Dat...   \n",
       "7           8                                     Data Scientist   \n",
       "8           9  URGENT: Data Scientist | Gurugram | 5 Days Wor...   \n",
       "9          10                                     Data Scientist   \n",
       "\n",
       "                                        JOB_LOCATION  \\\n",
       "0  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "1  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "2  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "3                                              Noida   \n",
       "4                                  Temp. WFH - Noida   \n",
       "5  Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...   \n",
       "6                                        Delhi / NCR   \n",
       "7                                   Gurgaon/Gurugram   \n",
       "8                                   Gurgaon/Gurugram   \n",
       "9                                              Noida   \n",
       "\n",
       "                    COMPANY_NAME   EXP_REQ  \n",
       "0                         Vicara   1-3 Yrs  \n",
       "1                 Okda Solutions  7-10 Yrs  \n",
       "2                   Primo Hiring   1-3 Yrs  \n",
       "3                 Navikenz India   3-8 Yrs  \n",
       "4                   NGI Ventures   1-5 Yrs  \n",
       "5           torcai digital media   2-7 Yrs  \n",
       "6           Tatras Data Services   2-4 Yrs  \n",
       "7  Americana Restaurants (india)   3-8 Yrs  \n",
       "8                     Digilytics   2-5 Yrs  \n",
       "9    Alliance Recruitment Agency   3-4 Yrs  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'SERIAL_NUM':range(1,11,1),'JOB_TITLE':job_title,'JOB_LOCATION':job_location,'COMPANY_NAME':company_name,'EXP_REQ':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9c77640",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "870f60cd",
   "metadata": {},
   "source": [
    "# Question no 04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb7d5a",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dab551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Required , Libraries.\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e974fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sunglasses in the search bar.\n",
    "sun_glasses = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sun_glasses.send_keys(\"sunglasses\")\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()\n",
    "\n",
    "Brand_ = []\n",
    "ProductDescription_ = []\n",
    "Price_ = []\n",
    "Percentageoff_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6f935f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 40 40 40 40\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping ProductDescription\n",
    "productdes_tags=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in productdes_tags[0:]:\n",
    "    product=i.text\n",
    "    ProductDescription_.append(product)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0be5081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PAGE STARTS < HERE\n",
    "\n",
    "# searching for the next_page icon bar on the page no 01\n",
    "\n",
    "next_page = driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "next_page.send_keys(\"NEXT\")\n",
    "\n",
    "# Click On Search Icon , of page 1.\n",
    "search = driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def12c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 80 80 80 80\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name , of page 2.\n",
    "brand_tags2=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags2[0:]:\n",
    "    brand2=i.text\n",
    "    Brand_.append(brand2)\n",
    "    \n",
    "# scraping ProductDescription  , of page 2.\n",
    "productdes_tags2=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in productdes_tags2[0:]:\n",
    "    product2=i.text\n",
    "    ProductDescription_.append(product2)\n",
    "    \n",
    "# scraping Price  , of page 2.\n",
    "price_tags2=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags2[0:]:\n",
    "    price2=i.text\n",
    "    Price_.append(price2)\n",
    "    \n",
    "    \n",
    "\n",
    "# scraping Percentage OFF  , of page 2.\n",
    "percentageoff_tags2=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags2[0:]:\n",
    "    percentageoff2=i.text\n",
    "    Percentageoff_.append(percentageoff2)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58990249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PAGE STARTS < HERE\n",
    "\n",
    "# searching for the next_page icon bar on the page no 01\n",
    "\n",
    "next_page2 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\")\n",
    "next_page2.send_keys(\"NEXT\")\n",
    "\n",
    "# Click On Search Icon , of page 1.\n",
    "search2 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\")\n",
    "search2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1720f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scraping Brand Name , of page 2.\n",
    "brand_tags3=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags3[0:20]:\n",
    "    brand3=i.text\n",
    "    Brand_.append(brand3)\n",
    "    \n",
    "# scraping ProductDescription  , of page 2.\n",
    "productdes_tags3=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in productdes_tags3[0:20]:\n",
    "    product3=i.text\n",
    "    ProductDescription_.append(product3)\n",
    "    \n",
    "# scraping Price  , of page 2.\n",
    "price_tags3=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags3[0:20]:\n",
    "    price3=i.text\n",
    "    Price_.append(price3)\n",
    "    \n",
    "    \n",
    "\n",
    "# scraping Percentage OFF  , of page 2.\n",
    "percentageoff_tags3=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags3[0:20]:\n",
    "    percentageoff3=i.text\n",
    "    Percentageoff_.append(percentageoff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc2314d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>BRAND_Of_Product</th>\n",
       "      <th>PRODUCT_DES_DETAILS</th>\n",
       "      <th>PRICE_DETAILS</th>\n",
       "      <th>PERCENTAGEOFF_DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PC STAR</td>\n",
       "      <td>Riding Glasses, UV Protection Retro Square Sun...</td>\n",
       "      <td>₹393</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RESIST EYEWEAR</td>\n",
       "      <td>Polarized, Riding Glasses, UV Protection Wayfa...</td>\n",
       "      <td>₹1,399</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹149</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹439</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (54)</td>\n",
       "      <td>₹616</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection, Mirrored Wayfarer Sunglasses (54)</td>\n",
       "      <td>₹129</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹179</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Cat-eye Sunglasses (56)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>AISLIN</td>\n",
       "      <td>Polarized, UV Protection Round Sunglasses (51)</td>\n",
       "      <td>₹524</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Serial_Number BRAND_Of_Product  \\\n",
       "0               1          PC STAR   \n",
       "1               2   RESIST EYEWEAR   \n",
       "2               3        Elligator   \n",
       "3               4       LIZA ANGEL   \n",
       "4               5         Fastrack   \n",
       "..            ...              ...   \n",
       "95             96   ROZZETTA CRAFT   \n",
       "96             97             SRPM   \n",
       "97             98        Elligator   \n",
       "98             99         Fastrack   \n",
       "99            100           AISLIN   \n",
       "\n",
       "                                  PRODUCT_DES_DETAILS PRICE_DETAILS  \\\n",
       "0   Riding Glasses, UV Protection Retro Square Sun...          ₹393   \n",
       "1   Polarized, Riding Glasses, UV Protection Wayfa...        ₹1,399   \n",
       "2   UV Protection Cat-eye, Retro Square, Oval, Rou...          ₹149   \n",
       "3   Riding Glasses, Night Vision Spectacle Sunglas...          ₹129   \n",
       "4    UV Protection Rectangular Sunglasses (Free Size)          ₹439   \n",
       "..                                                ...           ...   \n",
       "95             UV Protection Wayfarer Sunglasses (54)          ₹616   \n",
       "96   UV Protection, Mirrored Wayfarer Sunglasses (54)          ₹129   \n",
       "97      UV Protection Wayfarer Sunglasses (Free Size)          ₹179   \n",
       "98              UV Protection Cat-eye Sunglasses (56)          ₹599   \n",
       "99     Polarized, UV Protection Round Sunglasses (51)          ₹524   \n",
       "\n",
       "   PERCENTAGEOFF_DISCOUNT  \n",
       "0                 73% off  \n",
       "1                 72% off  \n",
       "2                 75% off  \n",
       "3                 87% off  \n",
       "4                 45% off  \n",
       "..                    ...  \n",
       "95                75% off  \n",
       "96                87% off  \n",
       "97                82% off  \n",
       "98                40% off  \n",
       "99                79% off  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Serial_Number':range(1,101,1),'BRAND_Of_Product':Brand_,'PRODUCT_DES_DETAILS':ProductDescription_,'PRICE_DETAILS':Price_,'PERCENTAGEOFF_DISCOUNT':Percentageoff_})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "419a7753",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4707489f",
   "metadata": {},
   "source": [
    "# Question no 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d6474",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65526a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required , libraries.\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "250703eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Creating the empty list .\n",
    "rating_ = []\n",
    "review_summary_ = []\n",
    "full_review_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "374061cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the User-Defined Fn.\n",
    "\n",
    "def rate_review_full():\n",
    "    \n",
    "    #Scraping Ratings.\n",
    "    rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rating_tags[0:]:\n",
    "        rating=i.text\n",
    "        rating_.append(rating)\n",
    "    \n",
    "    # Scraping ProductDescription\n",
    "    review_summary_tags=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in review_summary_tags[0:]:\n",
    "        review=i.text\n",
    "        review_summary_.append(review)\n",
    "    \n",
    "    # Scraping Price\n",
    "    full_review_tags=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in full_review_tags[0:]:\n",
    "        fullreview=i.text\n",
    "        full_review_.append(fullreview)\n",
    "    \n",
    "    print(\"\\n\\n\",len(rating_),len(review_summary_),len(full_review_))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e00cc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10 10 10\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Scraping elements from the page-01 . (using User_def_fn)\n",
    "\n",
    "rate_review_full()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a9f53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Getting to the next_ page no 02 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\")\n",
    "    time.sleep(2)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[2]')\n",
    "    time.sleep(2)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69bc873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-02. (using User_def_fn)\n",
    "\n",
    "rate_review_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3be22dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 03 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(2)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "    time.sleep(2)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97e0839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30 30 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scraping elements from the page-03. (using User_def_fn)\n",
    "\n",
    "rate_review_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0016b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 04 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[5]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b1e488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40 40 40\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-04. (using User_def_fn)\n",
    "\n",
    "rate_review_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c109f17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 05 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[6]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e408d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50 50 50\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-05. (using User_def_fn)\n",
    "\n",
    "rate_review_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa23c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 06 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c7876e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60 60 60\n"
     ]
    }
   ],
   "source": [
    "#scraping elements from the page-06. (using User_def_fn)\n",
    "    \n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags[0:]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "    \n",
    "# Scraping ProductDescription\n",
    "review_summary_tags=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "for i in review_summary_tags[0:]:\n",
    "    review=i.text\n",
    "    review_summary_.append(review)\n",
    "    \n",
    "# Scraping Price\n",
    "full_review_tags=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "for i in full_review_tags[0:]:\n",
    "    fullreview=i.text\n",
    "    full_review_.append(fullreview)\n",
    "\n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1rdVr6 _1BLPMq\"]')\n",
    "for i in rating_tags[0:]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "    \n",
    "print(\"\\n\\n\",len(rating_),len(review_summary_),len(full_review_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5e1bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 07 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[8]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdde2b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70 70 70\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-07. (using User_def_fn)\n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags[0:1]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "\n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1rdVr6 _1BLPMq\"]')\n",
    "for i in rating_tags[0:1]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "    \n",
    "\n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags[1:]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "\n",
    "# Scraping ProductDescription\n",
    "review_summary_tags=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "for i in review_summary_tags[0:]:\n",
    "    review=i.text\n",
    "    review_summary_.append(review)\n",
    "    \n",
    "# Scraping Price\n",
    "full_review_tags=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "for i in full_review_tags[0:]:\n",
    "    fullreview=i.text\n",
    "    full_review_.append(fullreview)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\",len(rating_),len(review_summary_),len(full_review_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "803c7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 08 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "735e11cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80 80 80\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-08. (using User_def_fn)\n",
    "    \n",
    "rate_review_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e439248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 09 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4ce0673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90 90 90\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-09. (using User_def_fn)\n",
    "    \n",
    "rate_review_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67a4b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT\n"
     ]
    }
   ],
   "source": [
    "# Getting to the next page 10 , by clicking the Nextpage_Webelement_Icon's.\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    element = driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]')\n",
    "    time.sleep(5)\n",
    "    print(element.text)\n",
    "    \n",
    "element.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d543523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 100 100 100\n"
     ]
    }
   ],
   "source": [
    "# Scraping elements from the page-10. \n",
    "\n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags[0:8]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "\n",
    "# Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1rdVr6 _1BLPMq\"]')\n",
    "for i in rating_tags[0:1]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)\n",
    "    \n",
    "# .Scraping Ratings.\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags[-1:]:\n",
    "    rating=i.text\n",
    "    rating_.append(rating)  \n",
    "    \n",
    "\n",
    "# Scraping ProductDescription\n",
    "review_summary_tags=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "for i in review_summary_tags[0:]:\n",
    "    review=i.text\n",
    "    review_summary_.append(review)\n",
    "    \n",
    "# Scraping Price\n",
    "full_review_tags=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "for i in full_review_tags[0:]:\n",
    "    fullreview=i.text\n",
    "    full_review_.append(fullreview)\n",
    "    \n",
    "print(\"\\n\\n\",len(rating_),len(review_summary_),len(full_review_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f9577d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>RATING_Of_Product</th>\n",
       "      <th>REVIEW_SUMMARY</th>\n",
       "      <th>FULL_REVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Value-for-money</td>\n",
       "      <td>I'm Really happy with the product\\nDelivery wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>It's my first time to use iOS phone and I am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Simply Awesome\\n\\nI have upgraded from iPhone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Damn this phone is a blast . Upgraded from and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Worth the money’ starting first from its perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>Absolute rubbish!</td>\n",
       "      <td>Worst product delivered by Flipkart\\nAfter 10d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Flipkart honoured on time delivery, I have use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Serial_Number RATING_Of_Product         REVIEW_SUMMARY  \\\n",
       "0               1                 5         Simply awesome   \n",
       "1               2                 5       Perfect product!   \n",
       "2               3                 5    Best in the market!   \n",
       "3               4                 4        Value-for-money   \n",
       "4               5                 5     Highly recommended   \n",
       "..            ...               ...                    ...   \n",
       "95             96                 5               Terrific   \n",
       "96             97                 5    Best in the market!   \n",
       "97             98                 5       Perfect product!   \n",
       "98             99                 1      Absolute rubbish!   \n",
       "99            100                 5  Mind-blowing purchase   \n",
       "\n",
       "                                          FULL_REVIEW  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   I'm Really happy with the product\\nDelivery wa...  \n",
       "4   It's my first time to use iOS phone and I am l...  \n",
       "..                                                ...  \n",
       "95  Simply Awesome\\n\\nI have upgraded from iPhone ...  \n",
       "96  Damn this phone is a blast . Upgraded from and...  \n",
       "97  Worth the money’ starting first from its perfo...  \n",
       "98  Worst product delivered by Flipkart\\nAfter 10d...  \n",
       "99  Flipkart honoured on time delivery, I have use...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making DataFrame of the above data frm the flipkart.\n",
    "\n",
    "df=pd.DataFrame({'Serial_Number':range(1,101,1),'RATING_Of_Product':rating_,'REVIEW_SUMMARY':review_summary_,'FULL_REVIEW':full_review_})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41bc8bd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07692bc2",
   "metadata": {},
   "source": [
    "# Question no 06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71816c3",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sneakers” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sneakers. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sneakers.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef6942a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Required , Libraries.\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c08a9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sunglasses in the search bar.\n",
    "sneakers_ = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sneakers_.send_keys(\"sneakers\")\n",
    "\n",
    "# Clicking on the Search_icon.\n",
    "search = driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()\n",
    "\n",
    "# Creating an Empty Lists.\n",
    "Brand_ = []\n",
    "ProductDescription_ = []\n",
    "Price_ = []\n",
    "Percentageoff_ = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11150cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 40 0 40 40\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:41]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:41]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:41]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "      \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02994894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click On Search Icon , of page 1.\n",
    "search = driver.find_element(By.CLASS_NAME,\"_1LKTO3\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ed4668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 80 0 80 80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:41]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:41]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:41]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "      \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a7ebe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click On Search Icon , of page 1.\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3c0b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 101 0 101 101\n"
     ]
    }
   ],
   "source": [
    "# scraping Brand Name\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:21]:\n",
    "    brand=i.text\n",
    "    Brand_.append(brand)\n",
    "    \n",
    "# scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in price_tags[0:21]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "# scraping Percentage OFF\n",
    "percentageoff_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in percentageoff_tags[0:21]:\n",
    "    percentageoff=i.text\n",
    "    Percentageoff_.append(percentageoff)\n",
    "      \n",
    "print(\"\\n\\n\\n\",len(Brand_),len(ProductDescription_),len(Price_),len(Percentageoff_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2429911c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>BRAND_Of_Product</th>\n",
       "      <th>PRICE_DETAILS</th>\n",
       "      <th>PERCENTAGEOFF_DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Labbin</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Shozie</td>\n",
       "      <td>₹295</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SFR</td>\n",
       "      <td>₹209</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RapidBox</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SFR</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>U.S. POLO ASSN.</td>\n",
       "      <td>₹2,789</td>\n",
       "      <td>38% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>BIG FOX</td>\n",
       "      <td>₹774</td>\n",
       "      <td>69% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>DEXTURE</td>\n",
       "      <td>₹849</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>₹459</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>BIG FOX</td>\n",
       "      <td>₹774</td>\n",
       "      <td>69% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial_Number BRAND_Of_Product PRICE_DETAILS PERCENTAGEOFF_DISCOUNT\n",
       "0                1           Labbin          ₹399                60% off\n",
       "1                2           Shozie          ₹295                70% off\n",
       "2                3              SFR          ₹209                65% off\n",
       "3                4         RapidBox          ₹599                40% off\n",
       "4                5              SFR          ₹299                70% off\n",
       "..             ...              ...           ...                    ...\n",
       "96              97  U.S. POLO ASSN.        ₹2,789                38% off\n",
       "97              98          BIG FOX          ₹774                69% off\n",
       "98              99          DEXTURE          ₹849                73% off\n",
       "99             100     Robbie jones          ₹459                54% off\n",
       "100            101          BIG FOX          ₹774                69% off\n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Serial_Number':range(1,102,1),'BRAND_Of_Product':Brand_,'PRICE_DETAILS':Price_,'PERCENTAGEOFF_DISCOUNT':Percentageoff_})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a6b526a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce096b14",
   "metadata": {},
   "source": [
    "# Question no 07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b9cbd",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39fb35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Required , Libraries.\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the amazon page on automated chrome browser\n",
    "driver.get(\"https://www.amazon.in/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94c81884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering \"Laptop\" in the search bar.\n",
    "laptop_ = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "laptop_.send_keys(\"Laptop\")\n",
    "\n",
    "# Clicking the Search_Icon.\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()\n",
    "\n",
    "# Creating Empty List.\n",
    "Title_ = []\n",
    "Rating_ = []\n",
    "Price_ = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf3fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering the CPU-Type in the search bar.\n",
    "\n",
    "cpu_type = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[6]/li[11]/span/a/span\")\n",
    "cpu_type.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a43cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Scraping Brand Name\n",
    "title_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags[0:11]:\n",
    "    title=i.text\n",
    "    Title_.append(title)\n",
    "    \n",
    "print(len(Title_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c90659d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(5.0)', '(3.9)', '(4.7)', '(5.0)', '(1.0)', '(3.7)', '(4.7)', '(3.0)', '(1.0)', '(5.0)', '(4.2)']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Scraping ProductDescription\n",
    "ratings_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-base\"]')\n",
    "for i in ratings_tags[0:11]:\n",
    "    ratings=i.text\n",
    "    Rating_.append(ratings)\n",
    "    \n",
    "\n",
    "print(Rating_)\n",
    "print(len(Rating_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "520fa019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4,32,435', '1,21,115', '1,52,100', '1,81,833', '2,13,990', '1,49,990', '3,39,500', '2,29,990', '3,01,980', '4,29,990', '1,87,990']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in price_tags[0:11]:\n",
    "    price=i.text\n",
    "    Price_.append(price)\n",
    "    \n",
    "print( Price_)\n",
    "print(len(Price_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46f6bd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>TITLE_Of_Product</th>\n",
       "      <th>RATING_OF_PRODUCT</th>\n",
       "      <th>PRICE_DETAILS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer Predator Helios 500 Gaming Laptop (11Th G...</td>\n",
       "      <td>(5.0)</td>\n",
       "      <td>4,32,435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Acer Predator Helios 300 Gaming Laptop Intel C...</td>\n",
       "      <td>(3.9)</td>\n",
       "      <td>1,21,115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dell G15-5520 Gaming Laptop, i9-12900H, 16GB D...</td>\n",
       "      <td>(4.7)</td>\n",
       "      <td>1,52,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dell G15-5521 Gaming Laptop, i9-12900H, 16GB D...</td>\n",
       "      <td>(5.0)</td>\n",
       "      <td>1,81,833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ASUS Zenbook Pro 14 Duo OLED (2022) Dual Scree...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>2,13,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Acer Predator Helios 300 Gaming Laptop Intel C...</td>\n",
       "      <td>(3.7)</td>\n",
       "      <td>1,49,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>HP Omen 12th Gen Intel Core i9-12900HX 17.3 in...</td>\n",
       "      <td>(4.7)</td>\n",
       "      <td>3,39,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>ASUS ROG Strix Scar 15 (2022), 15.6\" (39.62 cm...</td>\n",
       "      <td>(3.0)</td>\n",
       "      <td>2,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>ASUS ROG Strix Scar 15 (2022), 15.6\" (39.62 cm...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>3,01,980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MSI Raider GE67HX, Intel 12th Gen. i9-12900HX,...</td>\n",
       "      <td>(5.0)</td>\n",
       "      <td>4,29,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial_Number                                   TITLE_Of_Product  \\\n",
       "0              1  Acer Predator Helios 500 Gaming Laptop (11Th G...   \n",
       "1              2  Acer Predator Helios 300 Gaming Laptop Intel C...   \n",
       "2              3  Dell G15-5520 Gaming Laptop, i9-12900H, 16GB D...   \n",
       "3              4  Dell G15-5521 Gaming Laptop, i9-12900H, 16GB D...   \n",
       "4              5  ASUS Zenbook Pro 14 Duo OLED (2022) Dual Scree...   \n",
       "5              6  Acer Predator Helios 300 Gaming Laptop Intel C...   \n",
       "6              7  HP Omen 12th Gen Intel Core i9-12900HX 17.3 in...   \n",
       "7              8  ASUS ROG Strix Scar 15 (2022), 15.6\" (39.62 cm...   \n",
       "8              9  ASUS ROG Strix Scar 15 (2022), 15.6\" (39.62 cm...   \n",
       "9             10  MSI Raider GE67HX, Intel 12th Gen. i9-12900HX,...   \n",
       "\n",
       "  RATING_OF_PRODUCT PRICE_DETAILS  \n",
       "0             (5.0)      4,32,435  \n",
       "1             (3.9)      1,21,115  \n",
       "2             (4.7)      1,52,100  \n",
       "3             (5.0)      1,81,833  \n",
       "4             (1.0)      2,13,990  \n",
       "5             (3.7)      1,49,990  \n",
       "6             (4.7)      3,39,500  \n",
       "7             (3.0)      2,29,990  \n",
       "8             (1.0)      3,01,980  \n",
       "9             (5.0)      4,29,990  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a DataFrame, of the above Datasets.\n",
    "\n",
    "df=pd.DataFrame({'Serial_Number':range(1,11,1),'TITLE_Of_Product':Title_[0:10],'RATING_OF_PRODUCT':Rating_[0:10],'PRICE_DETAILS':Price_[0:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a697567",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34e7a135",
   "metadata": {},
   "source": [
    "# Question no 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca4bfa",
   "metadata": {},
   "source": [
    ": Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1.First get the webpage https://www.azquotes.com/\n",
    "2. Click on Top Quotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19a1eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Rrequired, Libraries.\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d7e36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the Search_Icon Of \" TOP QUOTES \".\n",
    "topquotes = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\")\n",
    "topquotes.click()\n",
    "\n",
    "# Creating an Empty Lists.\n",
    "Quote_ = []\n",
    "Author_ = []\n",
    "Type_Of_Quotes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1704294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "# Page no 01......................................................\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags[0:101]:\n",
    "    quote=i.text\n",
    "    Quote_.append(quote)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags[0:101]:\n",
    "    author=i.text\n",
    "    Author_.append(author)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags[0:101]:\n",
    "    typequote=i.text\n",
    "    Type_Of_Quotes.append(typequote)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8b0fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 200\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 02......................................................\n",
    "nextpage1 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage1.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags1=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags1[0:101]:\n",
    "    quote1=i.text\n",
    "    Quote_.append(quote1)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags1=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags1[0:101]:\n",
    "    author1=i.text\n",
    "    Author_.append(author1)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags1=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags1[0:101]:\n",
    "    typequote1=i.text\n",
    "    Type_Of_Quotes.append(typequote1)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49397ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300 300\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 03......................................................\n",
    "nextpage2 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage2.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags2=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags2[0:101]:\n",
    "    quote2=i.text\n",
    "    Quote_.append(quote2)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags2=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags2[0:101]:\n",
    "    author2=i.text\n",
    "    Author_.append(author2)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags2=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags2[0:101]:\n",
    "    typequote2=i.text\n",
    "    Type_Of_Quotes.append(typequote2)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a9f4fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 400 400\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 04......................................................\n",
    "nextpage3 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage3.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags3=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags3[0:101]:\n",
    "    quote3=i.text\n",
    "    Quote_.append(quote3)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags3=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags3[0:101]:\n",
    "    author3=i.text\n",
    "    Author_.append(author3)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags3=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags3[0:101]:\n",
    "    typequote3=i.text\n",
    "    Type_Of_Quotes.append(typequote3)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17414746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500 500\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 05......................................................\n",
    "nextpage4 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage4.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags4=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags4[0:101]:\n",
    "    quote4=i.text\n",
    "    Quote_.append(quote4)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags4=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags4[0:101]:\n",
    "    author4=i.text\n",
    "    Author_.append(author4)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags4=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags4[0:101]:\n",
    "    typequote4=i.text\n",
    "    Type_Of_Quotes.append(typequote4)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a016e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 600 600\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 06......................................................\n",
    "nextpage5 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage5.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags5=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags5[0:101]:\n",
    "    quote5=i.text\n",
    "    Quote_.append(quote5)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags5=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags5[0:101]:\n",
    "    author5=i.text\n",
    "    Author_.append(author5)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags5=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags5[0:101]:\n",
    "    typequote5=i.text\n",
    "    Type_Of_Quotes.append(typequote5)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f191a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 700 700\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 07......................................................\n",
    "nextpage6 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage6.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags6=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags6[0:101]:\n",
    "    quote6=i.text\n",
    "    Quote_.append(quote6)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags6=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags6[0:101]:\n",
    "    author6=i.text\n",
    "    Author_.append(author6)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags6=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags6[0:101]:\n",
    "    typequote6=i.text\n",
    "    Type_Of_Quotes.append(typequote6)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b98ac602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 800 800\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 08......................................................\n",
    "nextpage7 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage7.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags7=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags7[0:101]:\n",
    "    quote7=i.text\n",
    "    Quote_.append(quote7)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags7=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags7[0:101]:\n",
    "    author7=i.text\n",
    "    Author_.append(author7)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags7=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags7[0:101]:\n",
    "    typequote7=i.text\n",
    "    Type_Of_Quotes.append(typequote7)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcd6fc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 900 900\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 09......................................................\n",
    "nextpage8 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage8.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags8=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags8[0:101]:\n",
    "    quote8=i.text\n",
    "    Quote_.append(quote8)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags8=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags8[0:101]:\n",
    "    author8=i.text\n",
    "    Author_.append(author8)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags8=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags8[0:101]:\n",
    "    typequote8=i.text\n",
    "    Type_Of_Quotes.append(typequote8)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a20008f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "#Next Page no 10......................................................\n",
    "nextpage9 = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\")\n",
    "nextpage9.click()\n",
    "\n",
    "# scraping Quote_\n",
    "quote_tags9=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags9[0:101]:\n",
    "    quote9=i.text\n",
    "    Quote_.append(quote9)\n",
    "    \n",
    "# scraping Author_\n",
    "author_tags9=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags9[0:101]:\n",
    "    author9=i.text\n",
    "    Author_.append(author9)\n",
    "    \n",
    "# scraping Type_Of_Quotes\n",
    "typequote_tags9=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in typequote_tags9[0:101]:\n",
    "    typequote9=i.text\n",
    "    Type_Of_Quotes.append(typequote9)\n",
    "    \n",
    "print(len(Quote_),len(Author_),len(Type_Of_Quotes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "501d1fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>QUOTE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>TYPE_OF_QUOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>Regret for the things we did can be tempered b...</td>\n",
       "      <td>Sydney J. Harris</td>\n",
       "      <td>Love, Inspirational, Motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>America... just a nation of two hundred millio...</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Gun, Two, Qualms About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>For every disciplined effort there is a multip...</td>\n",
       "      <td>Jim Rohn</td>\n",
       "      <td>Inspirational, Greatness, Best Effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>The spiritual journey is individual, highly pe...</td>\n",
       "      <td>Ram Dass</td>\n",
       "      <td>Spiritual, Truth, Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>The mind is not a vessel to be filled but a fi...</td>\n",
       "      <td>Plutarch</td>\n",
       "      <td>Inspirational, Leadership, Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial_Number                                              QUOTE  \\\n",
       "0                1  The essence of strategy is choosing what not t...   \n",
       "1                2  One cannot and must not try to erase the past ...   \n",
       "2                3  Patriotism means to stand by the country. It d...   \n",
       "3                4  Death is something inevitable. When a man has ...   \n",
       "4                5  You have to love a nation that celebrates its ...   \n",
       "..             ...                                                ...   \n",
       "995            996  Regret for the things we did can be tempered b...   \n",
       "996            997  America... just a nation of two hundred millio...   \n",
       "997            998  For every disciplined effort there is a multip...   \n",
       "998            999  The spiritual journey is individual, highly pe...   \n",
       "999           1000  The mind is not a vessel to be filled but a fi...   \n",
       "\n",
       "                 AUTHOR                             TYPE_OF_QUOTE  \n",
       "0        Michael Porter  Essence, Deep Thought, Transcendentalism  \n",
       "1            Golda Meir                 Inspiration, Past, Trying  \n",
       "2    Theodore Roosevelt                       Country, Peace, War  \n",
       "3        Nelson Mandela        Inspirational, Motivational, Death  \n",
       "4          Erma Bombeck              4th Of July, Food, Patriotic  \n",
       "..                  ...                                       ...  \n",
       "995    Sydney J. Harris         Love, Inspirational, Motivational  \n",
       "996  Hunter S. Thompson                    Gun, Two, Qualms About  \n",
       "997            Jim Rohn     Inspirational, Greatness, Best Effort  \n",
       "998            Ram Dass                    Spiritual, Truth, Yoga  \n",
       "999            Plutarch      Inspirational, Leadership, Education  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a DataFrame for the Above Data.\n",
    "\n",
    "df=pd.DataFrame({'Serial_Number':range(1,1001,1),'QUOTE':Quote_[:1001],'AUTHOR':Author_[:1001],'TYPE_OF_QUOTE':Type_Of_Quotes[:1001]})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd4dfc75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac03d0bf",
   "metadata": {},
   "source": [
    "# Question no 09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c4a22",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6ce841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Required, Libraries.\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64b0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the gk_option.\n",
    "\n",
    "gk_option = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div/div[1]/div/div[6]/div/div[1]/header/div[3]/ul/li[9]/a\")\n",
    "gk_option.click()\n",
    "\n",
    "\n",
    "# Create the Empty Lists.\n",
    "PmName_ = []\n",
    "Born_Dead_ = []\n",
    "Term_Of_Office_ = []\n",
    "Remarks_ = []\n",
    "\n",
    "# Click on the \"List of all Prime Ministers of India\"\n",
    "\n",
    "list_Of_PM = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/div/div[10]/div/div/ul/li[2]/a\")\n",
    "list_Of_PM.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ca5cd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 18 18 18\n"
     ]
    }
   ],
   "source": [
    "#...................For PM_#01\n",
    "\n",
    "# scraping PmName_1\n",
    "PmName_tags1=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[2]/td[2]/p/strong/a')\n",
    "for i in PmName_tags1[0:1]:\n",
    "    PmName1=i.text\n",
    "    PmName_.append(PmName1)\n",
    "    \n",
    "# scraping Born_Dead_1\n",
    "borndead_tags1=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[2]/td[3]/p')\n",
    "for i in borndead_tags1[0:1]:\n",
    "    borndead1=i.text\n",
    "    Born_Dead_.append(borndead1)\n",
    "    \n",
    "# scraping Term_Of_Office_1\n",
    "termoffice_tags1=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[2]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags1[0:1]:\n",
    "    termoffice1=i.text\n",
    "    Term_Of_Office_.append(termoffice1)\n",
    "    \n",
    "# scraping Remarks_1\n",
    "remarks_tags1=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[2]/td[5]')\n",
    "for i in remarks_tags1[0:1]:\n",
    "    remarks1=i.text\n",
    "    Remarks_.append(remarks1)\n",
    "\n",
    "    \n",
    "#...................For PM_#02\n",
    "\n",
    "# scraping PmName_2\n",
    "PmName_tags2=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[3]/td[2]')\n",
    "for i in PmName_tags2[0:1]:\n",
    "    PmName2=i.text\n",
    "    PmName_.append(PmName2)\n",
    "\n",
    "# scraping Born_Dead_2\n",
    "borndead_tags2=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[3]/td[3]')\n",
    "for i in borndead_tags2[0:1]:\n",
    "    borndead2=i.text\n",
    "    Born_Dead_.append(borndead2)\n",
    "        \n",
    "# scraping Term_Of_Office_2\n",
    "termoffice_tags2=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[3]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags2[0:1]:\n",
    "    termoffice2=i.text\n",
    "    Term_Of_Office_.append(termoffice2)\n",
    "\n",
    "# scraping Remarks_2\n",
    "remarks_tags2=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[3]/td[5]')\n",
    "for i in remarks_tags2[0:1]:\n",
    "    remarks2=i.text\n",
    "    Remarks_.append(remarks2)  \n",
    "    \n",
    "\n",
    "#...................For PM_#03\n",
    "\n",
    "# scraping PmName_3\n",
    "PmName_tags3=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[4]/td[2]')\n",
    "for i in PmName_tags3[0:1]:\n",
    "    PmName3=i.text\n",
    "    PmName_.append(PmName3)\n",
    "    \n",
    "# scraping Born_Dead_3\n",
    "borndead_tags3=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[4]/td[3]')\n",
    "for i in borndead_tags3[0:1]:\n",
    "    borndead3=i.text\n",
    "    Born_Dead_.append(borndead3)\n",
    "    \n",
    "# scraping Term_Of_Office_3\n",
    "termoffice_tags3=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[4]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags3[0:1]:\n",
    "    termoffice3=i.text\n",
    "    Term_Of_Office_.append(termoffice3)\n",
    "    \n",
    "# scraping Remarks_3\n",
    "remarks_tags3=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[4]/td[5]')\n",
    "for i in remarks_tags3[0:1]:\n",
    "    remarks3=i.text\n",
    "    Remarks_.append(remarks3)\n",
    "\n",
    "    \n",
    "#...................For PM_#04\n",
    "\n",
    "# scraping PmName_4\n",
    "PmName_tags4=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[5]/td[2]')\n",
    "for i in PmName_tags4[0:1]:\n",
    "    PmName4=i.text\n",
    "    PmName_.append(PmName4)\n",
    "\n",
    "# scraping Born_Dead_4\n",
    "borndead_tags4=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[5]/td[3]')\n",
    "for i in borndead_tags4[0:1]:\n",
    "    borndead4=i.text\n",
    "    Born_Dead_.append(borndead4)\n",
    "        \n",
    "# scraping Term_Of_Office_4\n",
    "termoffice_tags4=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[5]/td[4]/p[1]')\n",
    "for i in termoffice_tags4[0:1]:\n",
    "    termoffice4=i.text\n",
    "    Term_Of_Office_.append(termoffice4)\n",
    "\n",
    "# scraping Remarks_4\n",
    "remarks_tags4=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[5]/td[5]/p')\n",
    "for i in remarks_tags4[0:1]:\n",
    "    remarks4=i.text\n",
    "    Remarks_.append(remarks4)  \n",
    "\n",
    "    \n",
    "#...................For PM_#05\n",
    "\n",
    "# scraping PmName_5\n",
    "PmName_tags5=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[6]/td[2]/p/strong/a')\n",
    "for i in PmName_tags5[0:1]:\n",
    "    PmName5=i.text\n",
    "    PmName_.append(PmName5)\n",
    "    \n",
    "# scraping Born_Dead_5\n",
    "borndead_tags5=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[6]/td[3]/p')\n",
    "for i in borndead_tags5[0:1]:\n",
    "    borndead5=i.text\n",
    "    Born_Dead_.append(borndead5)\n",
    "    \n",
    "# scraping Term_Of_Office_5\n",
    "termoffice_tags5=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[6]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags5[0:1]:\n",
    "    termoffice5=i.text\n",
    "    Term_Of_Office_.append(termoffice5)\n",
    "    \n",
    "# scraping Remarks_5\n",
    "remarks_tags5=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[6]/td[5]/p')\n",
    "for i in remarks_tags5[0:1]:\n",
    "    remarks5=i.text\n",
    "    Remarks_.append(remarks5)\n",
    "\n",
    "    \n",
    "#...................For PM_#06\n",
    "\n",
    "# scraping PmName_6\n",
    "PmName_tags6=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[7]/td[2]/p')\n",
    "for i in PmName_tags6[0:1]:\n",
    "    PmName6=i.text\n",
    "    PmName_.append(PmName6)\n",
    "\n",
    "# scraping Born_Dead_6\n",
    "borndead_tags6=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[7]/td[3]/p')\n",
    "for i in borndead_tags6[0:1]:\n",
    "    borndead6=i.text\n",
    "    Born_Dead_.append(borndead6)\n",
    "        \n",
    "# scraping Term_Of_Office_6\n",
    "termoffice_tags6=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[7]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags6[0:1]:\n",
    "    termoffice6=i.text\n",
    "    Term_Of_Office_.append(termoffice6)\n",
    "\n",
    "# scraping Remarks_6\n",
    "remarks_tags6=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[7]/td[5]/p')\n",
    "for i in remarks_tags6[0:1]:\n",
    "    remarks6=i.text\n",
    "    Remarks_.append(remarks6)  \n",
    "    \n",
    "    \n",
    "#...................For PM_#07\n",
    "\n",
    "# scraping PmName_7\n",
    "PmName_tags7=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[8]/td[2]/p')\n",
    "for i in PmName_tags7[0:1]:\n",
    "    PmName7=i.text\n",
    "    PmName_.append(PmName7)\n",
    "    \n",
    "# scraping Born_Dead_7\n",
    "borndead_tags7=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[8]/td[3]/p')\n",
    "for i in borndead_tags7[0:1]:\n",
    "    borndead7=i.text\n",
    "    Born_Dead_.append(borndead7)\n",
    "    \n",
    "# scraping Term_Of_Office_7\n",
    "termoffice_tags7=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[8]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags7[0:1]:\n",
    "    termoffice7=i.text\n",
    "    Term_Of_Office_.append(termoffice7)\n",
    "    \n",
    "# scraping Remarks_7\n",
    "remarks_tags7=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[8]/td[5]/p')\n",
    "for i in remarks_tags7[0:1]:\n",
    "    remarks7=i.text\n",
    "    Remarks_.append(remarks7)\n",
    "\n",
    "    \n",
    "#...................For PM_#08\n",
    "\n",
    "# scraping PmName_8\n",
    "PmName_tags8=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[9]/td[2]/p/strong/a')\n",
    "for i in PmName_tags8[0:1]:\n",
    "    PmName8=i.text\n",
    "    PmName_.append(PmName8)\n",
    "\n",
    "# scraping Born_Dead_8\n",
    "borndead_tags8=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[9]/td[3]/p')\n",
    "for i in borndead_tags8[0:1]:\n",
    "    borndead8=i.text\n",
    "    Born_Dead_.append(borndead8)\n",
    "        \n",
    "# scraping Term_Of_Office_8\n",
    "termoffice_tags8=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[9]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags8[0:1]:\n",
    "    termoffice8=i.text\n",
    "    Term_Of_Office_.append(termoffice8)\n",
    "\n",
    "# scraping Remarks_8\n",
    "remarks_tags8=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[9]/td[5]/p')\n",
    "for i in remarks_tags8[0:1]:\n",
    "    remarks8=i.text\n",
    "    Remarks_.append(remarks8)  \n",
    "    \n",
    "    \n",
    "#...................For PM_#09\n",
    "\n",
    "# scraping PmName_9\n",
    "PmName_tags9=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[10]/td[2]/p/a/strong')\n",
    "for i in PmName_tags9[0:1]:\n",
    "    PmName9=i.text\n",
    "    PmName_.append(PmName9)\n",
    "    \n",
    "# scraping Born_Dead_9\n",
    "borndead_tags9=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[10]/td[3]/p')\n",
    "for i in borndead_tags9[0:1]:\n",
    "    borndead9=i.text\n",
    "    Born_Dead_.append(borndead9)\n",
    "    \n",
    "# scraping Term_Of_Office_9\n",
    "termoffice_tags9=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[10]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags9[0:1]:\n",
    "    termoffice9=i.text\n",
    "    Term_Of_Office_.append(termoffice9)\n",
    "    \n",
    "# scraping Remarks_9\n",
    "remarks_tags9=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[10]/td[5]/p')\n",
    "for i in remarks_tags9[0:1]:\n",
    "    remarks9=i.text\n",
    "    Remarks_.append(remarks9)\n",
    "\n",
    "    \n",
    "#...................For PM_#10\n",
    "\n",
    "# scraping PmName_2\n",
    "PmName_tags10=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[11]/td[2]/p')\n",
    "for i in PmName_tags10[0:1]:\n",
    "    PmName10=i.text\n",
    "    PmName_.append(PmName10)\n",
    "\n",
    "# scraping Born_Dead_10\n",
    "borndead_tags10=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[11]/td[3]/p')\n",
    "for i in borndead_tags10[0:1]:\n",
    "    borndead10=i.text\n",
    "    Born_Dead_.append(borndead10)\n",
    "        \n",
    "# scraping Term_Of_Office_10\n",
    "termoffice_tags10=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[11]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags10[0:1]:\n",
    "    termoffice10=i.text\n",
    "    Term_Of_Office_.append(termoffice10)\n",
    "\n",
    "# scraping Remarks_10\n",
    "remarks_tags10=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[11]/td[5]/p')\n",
    "for i in remarks_tags10[0:1]:\n",
    "    remarks10=i.text\n",
    "    Remarks_.append(remarks10)  \n",
    "    \n",
    "    \n",
    "#...................For PM_#11\n",
    "\n",
    "# scraping PmName_11\n",
    "PmName_tags11=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[12]/td[2]/p/strong/a')\n",
    "for i in PmName_tags11[0:1]:\n",
    "    PmName11=i.text\n",
    "    PmName_.append(PmName11)\n",
    "    \n",
    "# scraping Born_Dead_11\n",
    "borndead_tags11=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[12]/td[3]/p')\n",
    "for i in borndead_tags11[0:1]:\n",
    "    borndead11=i.text\n",
    "    Born_Dead_.append(borndead11)\n",
    "    \n",
    "# scraping Term_Of_Office_11\n",
    "termoffice_tags11=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[12]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags11[0:1]:\n",
    "    termoffice11=i.text\n",
    "    Term_Of_Office_.append(termoffice11)\n",
    "    \n",
    "# scraping Remarks_11\n",
    "remarks_tags11=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[12]/td[5]/p')\n",
    "for i in remarks_tags11[0:1]:\n",
    "    remarks11=i.text\n",
    "    Remarks_.append(remarks11)\n",
    "\n",
    "    \n",
    "#...................For PM_#12\n",
    "\n",
    "# scraping PmName_12\n",
    "PmName_tags12=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[13]/td[2]/p')\n",
    "for i in PmName_tags12[0:1]:\n",
    "    PmName12=i.text\n",
    "    PmName_.append(PmName12)\n",
    "\n",
    "# scraping Born_Dead_12\n",
    "borndead_tags12=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[13]/td[3]/p')\n",
    "for i in borndead_tags12[0:1]:\n",
    "    borndead12=i.text\n",
    "    Born_Dead_.append(borndead12)\n",
    "        \n",
    "# scraping Term_Of_Office_12\n",
    "termoffice_tags12=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[13]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags12[0:1]:\n",
    "    termoffice12=i.text\n",
    "    Term_Of_Office_.append(termoffice12)\n",
    "\n",
    "# scraping Remarks_12\n",
    "remarks_tags12=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[13]/td[5]/p')\n",
    "for i in remarks_tags12[0:1]:\n",
    "    remarks12=i.text\n",
    "    Remarks_.append(remarks12)  \n",
    "    \n",
    "    \n",
    "#...................For PM_#13\n",
    "\n",
    "# scraping PmName_13\n",
    "PmName_tags13=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[14]/td[2]/p/strong/a')\n",
    "for i in PmName_tags13[0:1]:\n",
    "    PmName13=i.text\n",
    "    PmName_.append(PmName13)\n",
    "    \n",
    "# scraping Born_Dead_13\n",
    "borndead_tags13=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[14]/td[3]/p')\n",
    "for i in borndead_tags13[0:1]:\n",
    "    borndead13=i.text\n",
    "    Born_Dead_.append(borndead13)\n",
    "    \n",
    "# scraping Term_Of_Office_13\n",
    "termoffice_tags13=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[14]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags13[0:1]:\n",
    "    termoffice13=i.text\n",
    "    Term_Of_Office_.append(termoffice13)\n",
    "    \n",
    "# scraping Remarks_13\n",
    "remarks_tags13=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[14]/td[5]/p')\n",
    "for i in remarks_tags13[0:1]:\n",
    "    remarks13=i.text\n",
    "    Remarks_.append(remarks13)\n",
    "\n",
    "    \n",
    "#...................For PM_#14\n",
    "\n",
    "# scraping PmName_14\n",
    "PmName_tags14=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[15]/td[2]/p')\n",
    "for i in PmName_tags14[0:1]:\n",
    "    PmName14=i.text\n",
    "    PmName_.append(PmName14)\n",
    "\n",
    "# scraping Born_Dead_14\n",
    "borndead_tags14=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[15]/td[3]/p')\n",
    "for i in borndead_tags14[0:1]:\n",
    "    borndead14=i.text\n",
    "    Born_Dead_.append(borndead14)\n",
    "        \n",
    "# scraping Term_Of_Office_14\n",
    "termoffice_tags14=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[15]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags14[0:1]:\n",
    "    termoffice14=i.text\n",
    "    Term_Of_Office_.append(termoffice14)\n",
    "\n",
    "# scraping Remarks_14\n",
    "remarks_tags14=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[15]/td[5]/p')\n",
    "for i in remarks_tags14[0:1]:\n",
    "    remarks14=i.text\n",
    "    Remarks_.append(remarks14)  \n",
    "    \n",
    "    \n",
    "#...................For PM_#15\n",
    "\n",
    "# scraping PmName_15\n",
    "PmName_tags15=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[16]/td[2]/p')\n",
    "for i in PmName_tags15[0:1]:\n",
    "    PmName15=i.text\n",
    "    PmName_.append(PmName15)\n",
    "    \n",
    "# scraping Born_Dead_15\n",
    "borndead_tags15=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[16]/td[3]/p')\n",
    "for i in borndead_tags15[0:1]:\n",
    "    borndead15=i.text\n",
    "    Born_Dead_.append(borndead15)\n",
    "    \n",
    "# scraping Term_Of_Office_15\n",
    "termoffice_tags15=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[16]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags15[0:1]:\n",
    "    termoffice15=i.text\n",
    "    Term_Of_Office_.append(termoffice15)\n",
    "    \n",
    "# scraping Remarks_15\n",
    "remarks_tags15=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[16]/td[5]/p')\n",
    "for i in remarks_tags15[0:1]:\n",
    "    remarks15=i.text\n",
    "    Remarks_.append(remarks15)\n",
    "\n",
    "    \n",
    "#...................For PM_#16\n",
    "\n",
    "# scraping PmName_16\n",
    "PmName_tags16=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[17]/td[2]/p/a/strong')\n",
    "for i in PmName_tags16[0:1]:\n",
    "    PmName16=i.text\n",
    "    PmName_.append(PmName16)\n",
    "\n",
    "# scraping Born_Dead_16\n",
    "borndead_tags16=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[17]/td[3]/p')\n",
    "for i in borndead_tags16[0:1]:\n",
    "    borndead16=i.text\n",
    "    Born_Dead_.append(borndead16)\n",
    "        \n",
    "# scraping Term_Of_Office_16\n",
    "termoffice_tags16=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[17]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags16[0:1]:\n",
    "    termoffice16=i.text\n",
    "    Term_Of_Office_.append(termoffice16)\n",
    "\n",
    "# scraping Remarks_16\n",
    "remarks_tags16=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[17]/td[5]/p')\n",
    "for i in remarks_tags16[0:1]:\n",
    "    remarks16=i.text\n",
    "    Remarks_.append(remarks16)  \n",
    "    \n",
    "    \n",
    "#...................For PM_#17\n",
    "\n",
    "# scraping PmName_17\n",
    "PmName_tags17=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[18]/td[2]/p/strong/a')\n",
    "for i in PmName_tags17[0:1]:\n",
    "    PmName17=i.text\n",
    "    PmName_.append(PmName17)\n",
    "    \n",
    "# scraping Born_Dead_17\n",
    "borndead_tags17=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[18]/td[3]/p')\n",
    "for i in borndead_tags17[0:1]:\n",
    "    borndead17=i.text\n",
    "    Born_Dead_.append(borndead17)\n",
    "    \n",
    "# scraping Term_Of_Office_17\n",
    "termoffice_tags17=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[18]/td[4]/p[1]/span')\n",
    "for i in termoffice_tags17[0:1]:\n",
    "    termoffice17=i.text\n",
    "    Term_Of_Office_.append(termoffice17)\n",
    "    \n",
    "# scraping Remarks_17\n",
    "remarks_tags17=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[18]/td[5]/p')\n",
    "for i in remarks_tags17[0:1]:\n",
    "    remarks17=i.text\n",
    "    Remarks_.append(remarks17)\n",
    "\n",
    "    \n",
    "#...................For PM_#18\n",
    "\n",
    "# scraping PmName_18\n",
    "PmName_tags18=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[19]/td[2]/p/a/strong')\n",
    "for i in PmName_tags18[0:1]:\n",
    "    PmName18=i.text\n",
    "    PmName_.append(PmName18)\n",
    "\n",
    "# scraping Born_Dead_18\n",
    "borndead_tags18=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[19]/td[3]/p')\n",
    "for i in borndead_tags18[0:1]:\n",
    "    borndead18=i.text\n",
    "    Born_Dead_.append(borndead18)\n",
    "        \n",
    "# scraping Term_Of_Office_18\n",
    "termoffice_tags18=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[19]/td[4]/p/span')\n",
    "for i in termoffice_tags18[0:1]:\n",
    "    termoffice18=i.text\n",
    "    Term_Of_Office_.append(termoffice18)\n",
    "\n",
    "# scraping Remarks_18\n",
    "remarks_tags18=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div/div[2]/div/div[1]/div/div/div/div[4]/span/div[2]/table/tbody/tr[19]/td[5]/p')\n",
    "for i in remarks_tags18[0:1]:\n",
    "    remarks18=i.text\n",
    "    Remarks_.append(remarks18)  \n",
    "    \n",
    "    \n",
    "print(len(PmName_),len(Born_Dead_),len(Term_Of_Office_),len(Remarks_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6238a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>Pm_Name</th>\n",
       "      <th>BORN_DEAD_DATES</th>\n",
       "      <th>TERM_OF_OFFICE</th>\n",
       "      <th>REMARKS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jawahar Lal Nehru</td>\n",
       "      <td>(1889–1964)</td>\n",
       "      <td>15 August 1947 to 27 May 1964</td>\n",
       "      <td>The first prime minister of India and the long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Gulzarilal Nanda (Acting)</td>\n",
       "      <td>(1898-1998)</td>\n",
       "      <td>27 May 1964 to 9 June 1964,</td>\n",
       "      <td>First acting PM of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lal Bahadur Shastri</td>\n",
       "      <td>(1904–1966)</td>\n",
       "      <td>9 June 1964 to 11 January 1966</td>\n",
       "      <td>He has given the slogan of 'Jai Jawan Jai Kisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gulzari Lal Nanda  (Acting)</td>\n",
       "      <td>(1898-1998)</td>\n",
       "      <td>11 January 1966 to 24 January 1966</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Indira Gandhi</td>\n",
       "      <td>(1917–1984)</td>\n",
       "      <td>24 January 1966 to 24 March 1977</td>\n",
       "      <td>First female Prime Minister of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Morarji Desai</td>\n",
       "      <td>(1896–1995)</td>\n",
       "      <td>24 March 1977 to  28 July 1979</td>\n",
       "      <td>Oldest to become PM (81 years old) and first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Charan Singh</td>\n",
       "      <td>(1902–1987)</td>\n",
       "      <td>28 July 1979 to 14 January 1980</td>\n",
       "      <td>Only PM who did not face the Parliament</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Indira Gandhi</td>\n",
       "      <td>(1917–1984)</td>\n",
       "      <td>14 January 1980 to 31 October 1984</td>\n",
       "      <td>The first lady who served as PM for the second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Rajiv Gandhi</td>\n",
       "      <td>(1944–1991)</td>\n",
       "      <td>31 October 1984 to 2 December 1989</td>\n",
       "      <td>Youngest to become PM (40 years old)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>V. P. Singh</td>\n",
       "      <td>(1931–2008)</td>\n",
       "      <td>2 December 1989 to 10 November 1990</td>\n",
       "      <td>First PM to step down after a vote of no confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Chandra Shekhar</td>\n",
       "      <td>(1927–2007)</td>\n",
       "      <td>10 November 1990 to 21 June 1991</td>\n",
       "      <td>He belongs to  Samajwadi Janata Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>P. V. Narasimha Rao</td>\n",
       "      <td>(1921–2004)</td>\n",
       "      <td>21 June 1991 to 16 May 1996</td>\n",
       "      <td>First PM from south India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Atal Bihari Vajpayee</td>\n",
       "      <td>(1924- 2018)</td>\n",
       "      <td>16 May 1996 to 1 June 1996</td>\n",
       "      <td>PM for shortest tenure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>H. D. Deve Gowda</td>\n",
       "      <td>(born 1933)</td>\n",
       "      <td>1 June 1996 to 21 April 1997</td>\n",
       "      <td>He belongs to  Janata Dal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Inder Kumar Gujral</td>\n",
       "      <td>(1919–2012)</td>\n",
       "      <td>21 April 1997 to 19 March 1998</td>\n",
       "      <td>------</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Atal Bihari Vajpayee</td>\n",
       "      <td>(1924-2018)</td>\n",
       "      <td>19 March 1998 to 22 May 2004</td>\n",
       "      <td>The first non-congress PM who completed a ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Manmohan Singh</td>\n",
       "      <td>(born 1932)</td>\n",
       "      <td>22 May 2004 to 26 May 2014</td>\n",
       "      <td>First Sikh PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Narendra Modi</td>\n",
       "      <td>(born 1950)</td>\n",
       "      <td>26 May 2014 - Present</td>\n",
       "      <td>4th Prime Minister of India who served two con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Serial_Number                      Pm_Name BORN_DEAD_DATES  \\\n",
       "0               1            Jawahar Lal Nehru     (1889–1964)   \n",
       "1               2    Gulzarilal Nanda (Acting)     (1898-1998)   \n",
       "2               3          Lal Bahadur Shastri     (1904–1966)   \n",
       "3               4  Gulzari Lal Nanda  (Acting)     (1898-1998)   \n",
       "4               5                Indira Gandhi     (1917–1984)   \n",
       "5               6                Morarji Desai     (1896–1995)   \n",
       "6               7                 Charan Singh     (1902–1987)   \n",
       "7               8                Indira Gandhi     (1917–1984)   \n",
       "8               9                 Rajiv Gandhi     (1944–1991)   \n",
       "9              10                  V. P. Singh     (1931–2008)   \n",
       "10             11              Chandra Shekhar     (1927–2007)   \n",
       "11             12          P. V. Narasimha Rao     (1921–2004)   \n",
       "12             13         Atal Bihari Vajpayee    (1924- 2018)   \n",
       "13             14             H. D. Deve Gowda     (born 1933)   \n",
       "14             15           Inder Kumar Gujral     (1919–2012)   \n",
       "15             16         Atal Bihari Vajpayee     (1924-2018)   \n",
       "16             17               Manmohan Singh     (born 1932)   \n",
       "17             18                Narendra Modi     (born 1950)   \n",
       "\n",
       "                         TERM_OF_OFFICE  \\\n",
       "0         15 August 1947 to 27 May 1964   \n",
       "1           27 May 1964 to 9 June 1964,   \n",
       "2        9 June 1964 to 11 January 1966   \n",
       "3    11 January 1966 to 24 January 1966   \n",
       "4      24 January 1966 to 24 March 1977   \n",
       "5       24 March 1977 to  28 July 1979    \n",
       "6       28 July 1979 to 14 January 1980   \n",
       "7    14 January 1980 to 31 October 1984   \n",
       "8    31 October 1984 to 2 December 1989   \n",
       "9   2 December 1989 to 10 November 1990   \n",
       "10     10 November 1990 to 21 June 1991   \n",
       "11          21 June 1991 to 16 May 1996   \n",
       "12           16 May 1996 to 1 June 1996   \n",
       "13         1 June 1996 to 21 April 1997   \n",
       "14      21 April 1997 to 19 March 1998    \n",
       "15        19 March 1998 to 22 May 2004    \n",
       "16        22 May 2004 to 26 May 2014      \n",
       "17                26 May 2014 - Present   \n",
       "\n",
       "                                              REMARKS  \n",
       "0   The first prime minister of India and the long...  \n",
       "1                            First acting PM of India  \n",
       "2   He has given the slogan of 'Jai Jawan Jai Kisa...  \n",
       "3                                                   -  \n",
       "4                First female Prime Minister of India  \n",
       "5   Oldest to become PM (81 years old) and first t...  \n",
       "6             Only PM who did not face the Parliament  \n",
       "7   The first lady who served as PM for the second...  \n",
       "8                Youngest to become PM (40 years old)  \n",
       "9   First PM to step down after a vote of no confi...  \n",
       "10              He belongs to  Samajwadi Janata Party  \n",
       "11                          First PM from south India  \n",
       "12                             PM for shortest tenure  \n",
       "13                          He belongs to  Janata Dal  \n",
       "14                                             ------  \n",
       "15   The first non-congress PM who completed a ful...  \n",
       "16                                      First Sikh PM  \n",
       "17  4th Prime Minister of India who served two con...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Data Frame for the above details.\n",
    "\n",
    "df=pd.DataFrame({'Serial_Number':range(1,19,1),'Pm_Name':PmName_,'BORN_DEAD_DATES':Born_Dead_,'TERM_OF_OFFICE':Term_Of_Office_,'REMARKS':Remarks_})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21e5864b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2503db96",
   "metadata": {},
   "source": [
    "# Question no 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe21c6",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name ,Description and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on leftside.\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72cb86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Required, Libraries.\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Lets first connect to the driver-\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\" https://www.motor1.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835b45a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting in to the , \"Three_Search_Bar\" search bar\n",
    "\n",
    "try:\n",
    "    threebar_Btn = driver.find_element(By.XPATH,'/html/body/div[3]/div[2]/div/div/div[1]/div')\n",
    "    time.sleep(1)\n",
    "    print(threebar_Btn.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    threebar_Btn = driver.find_element(By.XPATH,'//div[@class=\"m1-hamburger-button\"]')\n",
    "    time.sleep(1)\n",
    "    print(threebar_Btn.text)\n",
    "    \n",
    "threebar_Btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f63b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES\n"
     ]
    }
   ],
   "source": [
    "# Getting in to the,\"Feature_Bar\" search bar\n",
    "try:\n",
    "    feature_Btn = driver.find_element(By.XPATH,'/html/body/div[4]/div[1]/div[3]/ul/li[5]')\n",
    "    time.sleep(1)\n",
    "    print(feature_Btn.text)\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Exception Raised :\" ,e )\n",
    "    feature_Btn = driver.find_element(By.XPATH,'/html/body/div[4]/div[1]/div[3]/ul/li[5]/a')\n",
    "    time.sleep(1)\n",
    "    print(feature_Btn.text)\n",
    "    \n",
    "feature_Btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88397803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Click on the \" WOLRD  _ TOP _50_ EXPENSIVE - CARS \"\n",
    "\n",
    "car50_Btn = driver.find_element(By.XPATH,'/html/body/div[3]/div[7]/div/div[1]/div[1]/div[2]/div/div[1]/h3/a')\n",
    "car50_Btn.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Create the Empty Lists.\n",
    "Car_Name_ = []\n",
    "Description_ = []\n",
    "price_3 = []\n",
    "price_4 = []\n",
    "price_5 = []\n",
    "Price_ = []\n",
    "Price_ = price_3 + price_4 + price_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a6bc0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['Drako GTE', 'De Tomaso P72', 'Ferrari LaFerrari', 'Pagani Huayra', 'McLaren Elva', 'Czinger 21C', 'Ferrari Monza', 'Gordon Murray T.33', 'Koenigsegg Gemera', 'Zenvo TSR-S', 'Hennessey Venom F5', 'Bentley Bacalar', 'Hispano Suiza Carmen Boulogne', 'Bentley Mulliner Batur', 'Deus Vayanne', 'SSC Tuatara', 'Lotus Evija', 'Aston Martin Vulcan', 'Delage D12', 'McLaren Speedtail', 'Rimac Nevera', 'Pagani Utopia', 'Pininfarina Battista', 'Ferrari FXX K Evo', 'Gordon Murray T.50', 'Lamborghini Countach', 'Mercedes-AMG Project One', 'Aston Martin Victor', 'Hennessey Venom F5 Roadster', 'Koenigsegg Jesko', 'Aston Martin Valkyrie', 'W Motors Lykan Hypersport', 'McLaren Solus', 'Pagani Huayra Roadster BC', 'Bugatti Chiron Pur Sport', 'Lamborghini Sian', 'Koenigsegg CC850', 'Bugatti Chiron Super Sport 300+', 'Lamborghini Veneno', 'Bugatti Bolide', 'Bugatti Mistral', 'Pagani Huayra Imola', 'Bugatti Divo', 'SP Automotive Chaos', 'Pagani Codalunga', 'Mercedes-Maybach Exelero', 'Bugatti Centodieci', 'Rolls-Royce Sweptail', 'Bugatti La Voiture Noire', 'Rolls-Royce Boat Tail*']\n"
     ]
    }
   ],
   "source": [
    "# THE \" Car Name _ TOP _50 _ Expensive\"\n",
    "\n",
    "cname_tags = driver.find_elements(By.XPATH,'//h3[@class=\"subheader\"]')\n",
    "for i in cname_tags[:50]:\n",
    "    cname=i.text\n",
    "    Car_Name_.append(cname)\n",
    "\n",
    "print(len(Car_Name_))\n",
    "print(Car_Name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c3e0fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['Price: $1.2 Million', 'Price: $1.3 Million', 'Price: $1.4 Million']\n"
     ]
    }
   ],
   "source": [
    "# THE \" Car PRICE_ TOP _03 \"\n",
    "\n",
    "cprice_tags3 = driver.find_elements(By.XPATH,'//p[strong]')\n",
    "for i in cprice_tags3[:3]:\n",
    "    price=i.text\n",
    "    price_3.append(price)\n",
    "\n",
    "print(len(price_3))\n",
    "print(price_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd4d507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['Price: $1.4 Million']\n"
     ]
    }
   ],
   "source": [
    "# THE \" Car PRICE_ TOP _only _04th \"\n",
    "\n",
    "cprice_tags4 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[10]')\n",
    "for i in cprice_tags4[:1]:\n",
    "    price=i.text\n",
    "    price_4.append(price)\n",
    "\n",
    "print(len(price_4))\n",
    "print(price_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e37ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "['Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.8 Million', 'Price: $1.9 Million', 'Price: $1.9 Million', 'Price: $2.0 Million', 'Price: $2.0 Million', 'Price: $2.0 Million*', 'Price: $2.1 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.4 Million', 'Price: $2.5 Million', 'Price: $2.5 Million', 'Price: $2.6 Million', 'Price: $2.6 Million', 'Price: $2.6 Million', 'Price: $2.7 Million', 'Price: $3.0 Million', '$3.0 Million', 'Price: $3.0 Million', 'Price: $3.2 Million', 'Price: $3.4 Million', '$3.5 Million', 'Price: $3.5 Million', 'Price: $3.6 Million', 'Price: $3.6 million', 'Price: $3.7 Million', 'Price: $3.9 Million', 'Price: $4.5 Million', 'Price: $4.7 Million', 'Price: $5.0 Million', 'Price: $5.4 Million', 'Price: $5.8 Million', 'Price: $6.4 Million', 'Price: $7.4 Million', 'Price: $8.0 Million', 'Price: $9.0 Million', 'Price: $12.8 Million', 'Price: $13.4 Million', 'Price: $28.0 Million (est.)']\n"
     ]
    }
   ],
   "source": [
    "# THE \" Car PRICE_ TOP _5th to 50th \"\n",
    "\n",
    "cprice_tags5 = driver.find_elements(By.XPATH,'//p[strong]')\n",
    "for i in cprice_tags5[5:]:\n",
    "    price=i.text\n",
    "    price_5.append(price)\n",
    "\n",
    "print(len(price_5))\n",
    "print(price_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78b190ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Price: $1.2 Million', 'Price: $1.3 Million', 'Price: $1.4 Million', 'Price: $1.4 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.8 Million', 'Price: $1.9 Million', 'Price: $1.9 Million', 'Price: $2.0 Million', 'Price: $2.0 Million', 'Price: $2.0 Million*', 'Price: $2.1 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.4 Million', 'Price: $2.5 Million', 'Price: $2.5 Million', 'Price: $2.6 Million', 'Price: $2.6 Million', 'Price: $2.6 Million', 'Price: $2.7 Million', 'Price: $3.0 Million', '$3.0 Million', 'Price: $3.0 Million', 'Price: $3.2 Million', 'Price: $3.4 Million', '$3.5 Million', 'Price: $3.5 Million', 'Price: $3.6 Million', 'Price: $3.6 million', 'Price: $3.7 Million', 'Price: $3.9 Million', 'Price: $4.5 Million', 'Price: $4.7 Million', 'Price: $5.0 Million', 'Price: $5.4 Million', 'Price: $5.8 Million', 'Price: $6.4 Million', 'Price: $7.4 Million', 'Price: $8.0 Million', 'Price: $9.0 Million', 'Price: $12.8 Million', 'Price: $13.4 Million', 'Price: $28.0 Million (est.)']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "Price_ = price_3 + price_4 + price_5\n",
    "print(Price_)\n",
    "print(len(Price_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e04585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# THE \" Car Description_ TOP _50 \"\n",
    "\n",
    "descrip_tags1 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[5]')\n",
    "for i in descrip_tags1[0:]:\n",
    "    desc1=i.text\n",
    "    Description_.append(desc1)\n",
    "    \n",
    "descrip_tags2 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[7]')\n",
    "for i in descrip_tags2[0:]:\n",
    "    desc2=i.text\n",
    "    Description_.append(desc2)\n",
    "    \n",
    "descrip_tags3 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[9]')\n",
    "for i in descrip_tags3[0:]:\n",
    "    desc3=i.text\n",
    "    Description_.append(desc3)\n",
    "\n",
    "\n",
    "descrip_tags4 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[11]')\n",
    "for i in descrip_tags4[0:]:\n",
    "    desc4=i.text\n",
    "    Description_.append(desc4)\n",
    "    \n",
    "descrip_tags5 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[13]')\n",
    "for i in descrip_tags5[0:]:\n",
    "    desc5=i.text\n",
    "    Description_.append(desc5)\n",
    "    \n",
    "descrip_tags6 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[15]')\n",
    "for i in descrip_tags6[0:]:\n",
    "    desc6=i.text\n",
    "    Description_.append(desc6)\n",
    "    \n",
    "descrip_tags7 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[17]')\n",
    "for i in descrip_tags7[0:]:\n",
    "    desc7=i.text\n",
    "    Description_.append(desc7)\n",
    "    \n",
    "descrip_tags8 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[19]')\n",
    "for i in descrip_tags8[0:]:\n",
    "    desc8=i.text\n",
    "    Description_.append(desc8)\n",
    "    \n",
    "descrip_tags9 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[21]')\n",
    "for i in descrip_tags9[0:]:\n",
    "    desc9=i.text\n",
    "    Description_.append(desc9)\n",
    "\n",
    "\n",
    "descrip_tags10 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[23]')\n",
    "for i in descrip_tags10[0:]:\n",
    "    desc10=i.text\n",
    "    Description_.append(desc10)\n",
    "    \n",
    "descrip_tags11 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[25]')\n",
    "for i in descrip_tags11[0:]:\n",
    "    desc11=i.text\n",
    "    Description_.append(desc11)\n",
    "    \n",
    "descrip_tags12 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[27]')\n",
    "for i in descrip_tags12[0:]:\n",
    "    desc12=i.text\n",
    "    Description_.append(desc12)\n",
    "    \n",
    "    \n",
    "descrip_tags13 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[29]')\n",
    "for i in descrip_tags13[0:]:\n",
    "    desc13=i.text\n",
    "    Description_.append(desc13)\n",
    "    \n",
    "descrip_tags14 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[31]')\n",
    "for i in descrip_tags14[0:]:\n",
    "    desc14=i.text\n",
    "    Description_.append(desc14)\n",
    "    \n",
    "descrip_tags15 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[33]')\n",
    "for i in descrip_tags15[0:]:\n",
    "    desc15=i.text\n",
    "    Description_.append(desc15)\n",
    "\n",
    "\n",
    "descrip_tags16 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[35]')\n",
    "for i in descrip_tags16[0:]:\n",
    "    desc16=i.text\n",
    "    Description_.append(desc16)\n",
    "    \n",
    "descrip_tags17 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[37]')\n",
    "for i in descrip_tags17[0:]:\n",
    "    desc17=i.text\n",
    "    Description_.append(desc17)\n",
    "    \n",
    "descrip_tags18 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[39]')\n",
    "for i in descrip_tags18[0:]:\n",
    "    desc18=i.text\n",
    "    Description_.append(desc18)\n",
    "    \n",
    "descrip_tags19 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[41]')\n",
    "for i in descrip_tags19[0:]:\n",
    "    desc19=i.text\n",
    "    Description_.append(desc19)\n",
    "    \n",
    "descrip_tags20 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[43]')\n",
    "for i in descrip_tags20[0:]:\n",
    "    desc20=i.text\n",
    "    Description_.append(desc20)\n",
    "    \n",
    "descrip_tags21 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[45]')\n",
    "for i in descrip_tags21[0:]:\n",
    "    desc21=i.text\n",
    "    Description_.append(desc21)\n",
    "\n",
    "\n",
    "descrip_tags22 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[47]')\n",
    "for i in descrip_tags22[0:]:\n",
    "    desc22=i.text\n",
    "    Description_.append(desc22)\n",
    "    \n",
    "descrip_tags23 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[49]')\n",
    "for i in descrip_tags23[0:]:\n",
    "    desc23=i.text\n",
    "    Description_.append(desc23)\n",
    "    \n",
    "descrip_tags24 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[51]')\n",
    "for i in descrip_tags24[0:]:\n",
    "    desc24=i.text\n",
    "    Description_.append(desc24)\n",
    "    \n",
    "    \n",
    "descrip_tags25 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[53]')\n",
    "for i in descrip_tags25[0:]:\n",
    "    desc25=i.text\n",
    "    Description_.append(desc25)\n",
    "    \n",
    "descrip_tags26 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[55]')\n",
    "for i in descrip_tags26[0:]:\n",
    "    desc26=i.text\n",
    "    Description_.append(desc26)\n",
    "    \n",
    "descrip_tags27 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[57]')\n",
    "for i in descrip_tags27[0:]:\n",
    "    desc27=i.text\n",
    "    Description_.append(desc27)\n",
    "\n",
    "\n",
    "descrip_tags28 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[59]')\n",
    "for i in descrip_tags28[0:]:\n",
    "    desc28=i.text\n",
    "    Description_.append(desc28)\n",
    "    \n",
    "descrip_tags29 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[61]')\n",
    "for i in descrip_tags29[0:]:\n",
    "    desc29=i.text\n",
    "    Description_.append(desc29)\n",
    "    \n",
    "descrip_tags30 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[63]')\n",
    "for i in descrip_tags30[0:]:\n",
    "    desc30=i.text\n",
    "    Description_.append(desc30)\n",
    "    \n",
    "descrip_tags31 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[65]')\n",
    "for i in descrip_tags31[0:]:\n",
    "    desc31=i.text\n",
    "    Description_.append(desc31)\n",
    "    \n",
    "descrip_tags32 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[67]')\n",
    "for i in descrip_tags32[0:]:\n",
    "    desc32=i.text\n",
    "    Description_.append(desc32)\n",
    "    \n",
    "descrip_tags33 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[69]')\n",
    "for i in descrip_tags33[0:]:\n",
    "    desc33=i.text\n",
    "    Description_.append(desc33)\n",
    "\n",
    "\n",
    "descrip_tags34 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[71]')\n",
    "for i in descrip_tags34[0:]:\n",
    "    desc34=i.text\n",
    "    Description_.append(desc34)\n",
    "    \n",
    "descrip_tags35 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[73]')\n",
    "for i in descrip_tags35[0:]:\n",
    "    desc35=i.text\n",
    "    Description_.append(desc35)\n",
    "    \n",
    "descrip_tags36 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[75]')\n",
    "for i in descrip_tags36[0:]:\n",
    "    desc36=i.text\n",
    "    Description_.append(desc36)\n",
    "    \n",
    "    \n",
    "descrip_tags37 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[77]')\n",
    "for i in descrip_tags37[0:]:\n",
    "    desc37=i.text\n",
    "    Description_.append(desc37)\n",
    "    \n",
    "descrip_tags38 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[79]')\n",
    "for i in descrip_tags38[0:]:\n",
    "    desc38=i.text\n",
    "    Description_.append(desc38)\n",
    "    \n",
    "descrip_tags39 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[81]')\n",
    "for i in descrip_tags39[0:]:\n",
    "    desc39=i.text\n",
    "    Description_.append(desc39)\n",
    "\n",
    "\n",
    "descrip_tags40 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[83]')\n",
    "for i in descrip_tags40[0:]:\n",
    "    desc40=i.text\n",
    "    Description_.append(desc40)\n",
    "    \n",
    "descrip_tags41 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[85]')\n",
    "for i in descrip_tags41[0:]:\n",
    "    desc41=i.text\n",
    "    Description_.append(desc41)\n",
    "    \n",
    "descrip_tags42 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[87]')\n",
    "for i in descrip_tags42[0:]:\n",
    "    desc42=i.text\n",
    "    Description_.append(desc42)\n",
    "    \n",
    "descrip_tags43 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[89]')\n",
    "for i in descrip_tags43[0:]:\n",
    "    desc43=i.text\n",
    "    Description_.append(desc43)\n",
    "    \n",
    "descrip_tags44 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[91]')\n",
    "for i in descrip_tags44[0:]:\n",
    "    desc44=i.text\n",
    "    Description_.append(desc44)\n",
    "    \n",
    "descrip_tags45 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[93]')\n",
    "for i in descrip_tags45[0:]:\n",
    "    desc45=i.text\n",
    "    Description_.append(desc45)\n",
    "\n",
    "\n",
    "descrip_tags46 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[95]')\n",
    "for i in descrip_tags46[0:]:\n",
    "    desc46=i.text\n",
    "    Description_.append(desc46)\n",
    "    \n",
    "descrip_tags47 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[97]')\n",
    "for i in descrip_tags47[0:]:\n",
    "    desc47=i.text\n",
    "    Description_.append(desc47)\n",
    "    \n",
    "descrip_tags48 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[99]')\n",
    "for i in descrip_tags48[0:]:\n",
    "    desc48=i.text\n",
    "    Description_.append(desc48)\n",
    "    \n",
    "descrip_tags49 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[101]')\n",
    "for i in descrip_tags49[0:]:\n",
    "    desc49=i.text\n",
    "    Description_.append(desc49)\n",
    "    \n",
    "descrip_tags50 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[103]')\n",
    "for i in descrip_tags50[0:]:\n",
    "    desc50=i.text\n",
    "    Description_.append(desc50)\n",
    "\n",
    "    \n",
    "print(len(Description_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ec71b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_Number</th>\n",
       "      <th>CAR_Name</th>\n",
       "      <th>PRICE_Of_CAR</th>\n",
       "      <th>CAR_Discription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Drako GTE</td>\n",
       "      <td>Price: $1.2 Million</td>\n",
       "      <td>The Drako GTE is a super sedan in every sense ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>De Tomaso P72</td>\n",
       "      <td>Price: $1.3 Million</td>\n",
       "      <td>The De Tomaso P72 is basically the definition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ferrari LaFerrari</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "      <td>At $1.4 million new, the Ferrari LaFerrari is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pagani Huayra</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "      <td>Inarguably one of the prettiest cars on this l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>McLaren Elva</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>The McLaren Elva is one of the latest addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Czinger 21C</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>You might not know the name Czinger yet, but t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Ferrari Monza</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>Much like the roof-less McLaren Elva, the Ferr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Gordon Murray T.33</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>The second and slightly more affordable superc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Koenigsegg Gemera</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>One of two Koenigsegg models on this list, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Zenvo TSR-S</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>Hailing from Denmark, the Zenvo TSR-S debuted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Hennessey Venom F5</td>\n",
       "      <td>Price: $1.8 Million</td>\n",
       "      <td>The Hennessey Venom GT was a record-breaker, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Bentley Bacalar</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "      <td>With just 12 total units produced, the Bentley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Hispano Suiza Carmen Boulogne</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "      <td>To call the Hispano Suiza Carmen Boulogne beau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bentley Mulliner Batur</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "      <td>The electric onslaught is coming. Bentley says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Deus Vayanne</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "      <td>The Deus Vayanne may not be a household name (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>SSC Tuatara</td>\n",
       "      <td>Price: $2.0 Million*</td>\n",
       "      <td>Although initially cloaked in controversy, SSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Lotus Evija</td>\n",
       "      <td>Price: $2.1 Million</td>\n",
       "      <td>With a new Emira sports car and an Eletre elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Aston Martin Vulcan</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "      <td>As with a few other cars on this list, the Ast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Delage D12</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "      <td>You may have heard of Delage before. In the ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>McLaren Speedtail</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "      <td>What would you pay for the fastest production ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Rimac Nevera</td>\n",
       "      <td>Price: $2.4 Million</td>\n",
       "      <td>The Rimac Nevera takes the title of most expen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Pagani Utopia</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "      <td>First came the Zonda, then the Huayra, and now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Pininfarina Battista</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "      <td>Aptly named after the company’s founder, Batti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Ferrari FXX K Evo</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "      <td>Sure, you could buy a normal LaFerrari (which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Gordon Murray T.50</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "      <td>If the name Gordon Murray sounds familiar, it’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Lamborghini Countach</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "      <td>The name Countach may be iconic, but is it wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Mercedes-AMG Project One</td>\n",
       "      <td>Price: $2.7 Million</td>\n",
       "      <td>Mercedes has promised a production version of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Aston Martin Victor</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "      <td>The folks at Aston Martin know a thing or two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Hennessey Venom F5 Roadster</td>\n",
       "      <td>$3.0 Million</td>\n",
       "      <td>The same Hennessey Venom F5 hypercar we all kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Koenigsegg Jesko</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "      <td>The Koenigsegg Jesko, apart from being the fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Aston Martin Valkyrie</td>\n",
       "      <td>Price: $3.2 Million</td>\n",
       "      <td>The upcoming Aston Martin hybrid hypercar hasn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>W Motors Lykan Hypersport</td>\n",
       "      <td>Price: $3.4 Million</td>\n",
       "      <td>Dubai-based W Motors shocked the world with it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>McLaren Solus</td>\n",
       "      <td>$3.5 Million</td>\n",
       "      <td>One seat, 829 horsepower, and a top speed of o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Pagani Huayra Roadster BC</td>\n",
       "      <td>Price: $3.5 Million</td>\n",
       "      <td>Following Pagani’s past playbook, a roadster v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Bugatti Chiron Pur Sport</td>\n",
       "      <td>Price: $3.6 Million</td>\n",
       "      <td>Another showstopper from the Geneva Motor Show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>Lamborghini Sian</td>\n",
       "      <td>Price: $3.6 million</td>\n",
       "      <td>In several ways, the Sian represents a bridge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>Koenigsegg CC850</td>\n",
       "      <td>Price: $3.7 Million</td>\n",
       "      <td>The Koenigsegg CC850 was a surprise to be sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>Bugatti Chiron Super Sport 300+</td>\n",
       "      <td>Price: $3.9 Million</td>\n",
       "      <td>Earlier this year, Bugatti captured the collec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>Lamborghini Veneno</td>\n",
       "      <td>Price: $4.5 Million</td>\n",
       "      <td>Lamborghini built just 14 examples of the Aven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>Bugatti Bolide</td>\n",
       "      <td>Price: $4.7 Million</td>\n",
       "      <td>Produced in extremely limited numbers atop the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>Bugatti Mistral</td>\n",
       "      <td>Price: $5.0 Million</td>\n",
       "      <td>The Bugatti Mistral sends the iconic W16 engin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>Pagani Huayra Imola</td>\n",
       "      <td>Price: $5.4 Million</td>\n",
       "      <td>Even though the new Utopia marks the next big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>Bugatti Divo</td>\n",
       "      <td>Price: $5.8 Million</td>\n",
       "      <td>Among Bugatti’s recently debuted vehicles, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>SP Automotive Chaos</td>\n",
       "      <td>Price: $6.4 Million</td>\n",
       "      <td>SP Automotive (short for Spyros Panopoulos) is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>Pagani Codalunga</td>\n",
       "      <td>Price: $7.4 Million</td>\n",
       "      <td>The long list of pricey (new) Paganis ends wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>Mercedes-Maybach Exelero</td>\n",
       "      <td>Price: $8.0 Million</td>\n",
       "      <td>Like many others before it, the Mercedes-Benz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>Bugatti Centodieci</td>\n",
       "      <td>Price: $9.0 Million</td>\n",
       "      <td>Bugatti debuted the Centodieci at last year’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>Rolls-Royce Sweptail</td>\n",
       "      <td>Price: $12.8 Million</td>\n",
       "      <td>Rolls-Royce, expectedly, takes two of the top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>Bugatti La Voiture Noire</td>\n",
       "      <td>Price: $13.4 Million</td>\n",
       "      <td>With a price tag of $13.4 million, the one-off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Rolls-Royce Boat Tail*</td>\n",
       "      <td>Price: $28.0 Million (est.)</td>\n",
       "      <td>Rolls-Royce is back in the business of coachbu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Serial_Number                         CAR_Name  \\\n",
       "0               1                        Drako GTE   \n",
       "1               2                    De Tomaso P72   \n",
       "2               3                Ferrari LaFerrari   \n",
       "3               4                    Pagani Huayra   \n",
       "4               5                     McLaren Elva   \n",
       "5               6                      Czinger 21C   \n",
       "6               7                    Ferrari Monza   \n",
       "7               8               Gordon Murray T.33   \n",
       "8               9                Koenigsegg Gemera   \n",
       "9              10                      Zenvo TSR-S   \n",
       "10             11               Hennessey Venom F5   \n",
       "11             12                  Bentley Bacalar   \n",
       "12             13    Hispano Suiza Carmen Boulogne   \n",
       "13             14           Bentley Mulliner Batur   \n",
       "14             15                     Deus Vayanne   \n",
       "15             16                      SSC Tuatara   \n",
       "16             17                      Lotus Evija   \n",
       "17             18              Aston Martin Vulcan   \n",
       "18             19                       Delage D12   \n",
       "19             20                McLaren Speedtail   \n",
       "20             21                     Rimac Nevera   \n",
       "21             22                    Pagani Utopia   \n",
       "22             23             Pininfarina Battista   \n",
       "23             24                Ferrari FXX K Evo   \n",
       "24             25               Gordon Murray T.50   \n",
       "25             26             Lamborghini Countach   \n",
       "26             27         Mercedes-AMG Project One   \n",
       "27             28              Aston Martin Victor   \n",
       "28             29      Hennessey Venom F5 Roadster   \n",
       "29             30                 Koenigsegg Jesko   \n",
       "30             31            Aston Martin Valkyrie   \n",
       "31             32        W Motors Lykan Hypersport   \n",
       "32             33                    McLaren Solus   \n",
       "33             34        Pagani Huayra Roadster BC   \n",
       "34             35         Bugatti Chiron Pur Sport   \n",
       "35             36                 Lamborghini Sian   \n",
       "36             37                 Koenigsegg CC850   \n",
       "37             38  Bugatti Chiron Super Sport 300+   \n",
       "38             39               Lamborghini Veneno   \n",
       "39             40                   Bugatti Bolide   \n",
       "40             41                  Bugatti Mistral   \n",
       "41             42              Pagani Huayra Imola   \n",
       "42             43                     Bugatti Divo   \n",
       "43             44              SP Automotive Chaos   \n",
       "44             45                 Pagani Codalunga   \n",
       "45             46         Mercedes-Maybach Exelero   \n",
       "46             47               Bugatti Centodieci   \n",
       "47             48             Rolls-Royce Sweptail   \n",
       "48             49         Bugatti La Voiture Noire   \n",
       "49             50           Rolls-Royce Boat Tail*   \n",
       "\n",
       "                   PRICE_Of_CAR  \\\n",
       "0           Price: $1.2 Million   \n",
       "1           Price: $1.3 Million   \n",
       "2           Price: $1.4 Million   \n",
       "3           Price: $1.4 Million   \n",
       "4           Price: $1.7 Million   \n",
       "5           Price: $1.7 Million   \n",
       "6           Price: $1.7 Million   \n",
       "7           Price: $1.7 Million   \n",
       "8           Price: $1.7 Million   \n",
       "9           Price: $1.7 Million   \n",
       "10          Price: $1.8 Million   \n",
       "11          Price: $1.9 Million   \n",
       "12          Price: $1.9 Million   \n",
       "13          Price: $2.0 Million   \n",
       "14          Price: $2.0 Million   \n",
       "15         Price: $2.0 Million*   \n",
       "16          Price: $2.1 Million   \n",
       "17          Price: $2.3 Million   \n",
       "18          Price: $2.3 Million   \n",
       "19          Price: $2.3 Million   \n",
       "20          Price: $2.4 Million   \n",
       "21          Price: $2.5 Million   \n",
       "22          Price: $2.5 Million   \n",
       "23          Price: $2.6 Million   \n",
       "24          Price: $2.6 Million   \n",
       "25          Price: $2.6 Million   \n",
       "26          Price: $2.7 Million   \n",
       "27          Price: $3.0 Million   \n",
       "28                 $3.0 Million   \n",
       "29          Price: $3.0 Million   \n",
       "30          Price: $3.2 Million   \n",
       "31          Price: $3.4 Million   \n",
       "32                 $3.5 Million   \n",
       "33          Price: $3.5 Million   \n",
       "34          Price: $3.6 Million   \n",
       "35          Price: $3.6 million   \n",
       "36          Price: $3.7 Million   \n",
       "37          Price: $3.9 Million   \n",
       "38          Price: $4.5 Million   \n",
       "39          Price: $4.7 Million   \n",
       "40          Price: $5.0 Million   \n",
       "41          Price: $5.4 Million   \n",
       "42          Price: $5.8 Million   \n",
       "43          Price: $6.4 Million   \n",
       "44          Price: $7.4 Million   \n",
       "45          Price: $8.0 Million   \n",
       "46          Price: $9.0 Million   \n",
       "47         Price: $12.8 Million   \n",
       "48         Price: $13.4 Million   \n",
       "49  Price: $28.0 Million (est.)   \n",
       "\n",
       "                                      CAR_Discription  \n",
       "0   The Drako GTE is a super sedan in every sense ...  \n",
       "1   The De Tomaso P72 is basically the definition ...  \n",
       "2   At $1.4 million new, the Ferrari LaFerrari is ...  \n",
       "3   Inarguably one of the prettiest cars on this l...  \n",
       "4   The McLaren Elva is one of the latest addition...  \n",
       "5   You might not know the name Czinger yet, but t...  \n",
       "6   Much like the roof-less McLaren Elva, the Ferr...  \n",
       "7   The second and slightly more affordable superc...  \n",
       "8   One of two Koenigsegg models on this list, the...  \n",
       "9   Hailing from Denmark, the Zenvo TSR-S debuted ...  \n",
       "10  The Hennessey Venom GT was a record-breaker, t...  \n",
       "11  With just 12 total units produced, the Bentley...  \n",
       "12  To call the Hispano Suiza Carmen Boulogne beau...  \n",
       "13  The electric onslaught is coming. Bentley says...  \n",
       "14  The Deus Vayanne may not be a household name (...  \n",
       "15  Although initially cloaked in controversy, SSC...  \n",
       "16  With a new Emira sports car and an Eletre elec...  \n",
       "17  As with a few other cars on this list, the Ast...  \n",
       "18  You may have heard of Delage before. In the ea...  \n",
       "19  What would you pay for the fastest production ...  \n",
       "20  The Rimac Nevera takes the title of most expen...  \n",
       "21  First came the Zonda, then the Huayra, and now...  \n",
       "22  Aptly named after the company’s founder, Batti...  \n",
       "23  Sure, you could buy a normal LaFerrari (which ...  \n",
       "24  If the name Gordon Murray sounds familiar, it’...  \n",
       "25  The name Countach may be iconic, but is it wor...  \n",
       "26  Mercedes has promised a production version of ...  \n",
       "27  The folks at Aston Martin know a thing or two ...  \n",
       "28  The same Hennessey Venom F5 hypercar we all kn...  \n",
       "29  The Koenigsegg Jesko, apart from being the fas...  \n",
       "30  The upcoming Aston Martin hybrid hypercar hasn...  \n",
       "31  Dubai-based W Motors shocked the world with it...  \n",
       "32  One seat, 829 horsepower, and a top speed of o...  \n",
       "33  Following Pagani’s past playbook, a roadster v...  \n",
       "34  Another showstopper from the Geneva Motor Show...  \n",
       "35  In several ways, the Sian represents a bridge ...  \n",
       "36  The Koenigsegg CC850 was a surprise to be sure...  \n",
       "37  Earlier this year, Bugatti captured the collec...  \n",
       "38  Lamborghini built just 14 examples of the Aven...  \n",
       "39  Produced in extremely limited numbers atop the...  \n",
       "40  The Bugatti Mistral sends the iconic W16 engin...  \n",
       "41  Even though the new Utopia marks the next big ...  \n",
       "42  Among Bugatti’s recently debuted vehicles, the...  \n",
       "43  SP Automotive (short for Spyros Panopoulos) is...  \n",
       "44  The long list of pricey (new) Paganis ends wit...  \n",
       "45  Like many others before it, the Mercedes-Benz ...  \n",
       "46  Bugatti debuted the Centodieci at last year’s ...  \n",
       "47  Rolls-Royce, expectedly, takes two of the top ...  \n",
       "48  With a price tag of $13.4 million, the one-off...  \n",
       "49  Rolls-Royce is back in the business of coachbu...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Data Frame for the above details.\n",
    "\n",
    "df=pd.DataFrame({'Serial_Number':range(1,51,1),'CAR_Name':Car_Name_,'PRICE_Of_CAR':Price_,'CAR_Discription':Description_})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94ca306b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
